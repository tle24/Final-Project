---
title: "Modeling"
format: html
editor: visual
---

## Introduction

```{r, warning=FALSE, message=FALSE}
#library some packages
library(readr)
library(dplyr)
```


```{r}
#read in data
diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv", col_types = c("Diabetes_binary" = "factor", "HighBP" = "factor", "HighChol" = "factor", "CholCheck" = "factor", "Smoker" = "factor", "Stroke" = "factor", "HeartDiseaseorAttack" = "factor", "PhysActivity" = "factor", "Fruits" = "factor", "Veggies" = "factor", "HvyAlcoholConsump" = "factor", "AnyHealthcare" = "factor", "NoDocbcCost" = "factor", "GenHlth" = "factor", "DiffWalk" = "factor", "Sex" = "factor", "Age" = "factor", "Education" = "factor", "Income" = "factor")) |>
  mutate(Diabetes_binary = recode(Diabetes_binary, "0.0" = "No diabetes", "1.0" = "Diabetes")) |>
  mutate(HighBP = recode(HighBP, "0.0" = "no high BP", "1.0" = "high BP")) |>
  mutate(HighChol = recode(HighChol, "0.0" = "no high cholesterol", "1.0" = "high cholesterol")) |>
  mutate(CholCheck = recode(CholCheck, "0.0" = "no cholesterol check", "1.0" = "cholesterol check")) |>
  mutate(Smoker = recode(Smoker, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(Stroke = recode(Stroke, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(HeartDiseaseorAttack = recode(HeartDiseaseorAttack, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(PhysActivity = recode(PhysActivity, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(Fruits = recode(Fruits, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(Veggies = recode(Veggies, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(HvyAlcoholConsump = recode(HvyAlcoholConsump, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(AnyHealthcare = recode(AnyHealthcare, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(NoDocbcCost = recode(NoDocbcCost, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(Fruits = recode(Fruits, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(GenHlth = recode(GenHlth, "1.0" = "excellent", "2.0" = "very good", "3.0" = "good", "4.0" = "fair", "5.0" = "poor")) |>
  mutate(DiffWalk = recode(DiffWalk, "0.0" = "No", "1.0" = "Yes")) |>
  mutate(Sex = recode(Sex, "0.0" = "female", "1.0" = "male")) |>
  mutate(Age = recode(Age, "1.0" = "18-24", "2.0" = "25-29", "3.0" = "30-34", "4.0" = "35-39", "5.0" = "40-44", "6.0" = "45-49", "7.0" = "50-54", "8.0" = "55-59", "9.0" = "60-64", "10.0" = "65-69", "11.0" = "70-74", "12.0" = "75-79", "13.0" = "80 or older")) |>
  mutate(Education = recode(Education, "1.0" = "never attended school", "2.0" = "elementary", "3.0" = "junior high", "4.0" = "high school", "5.0" = "undergraduate", "6.0" = "postgraduate")) |>
  mutate(Income = recode(Income, "1.0" = "less than $10,000", "2.0" = "less than $15,000", "3.0" = "less than $20,000", "4.0" = "less than $$25,000", "5.0" = "less than $35,000", "6.0" = "less than $50,000", "7.0" = "less than $75,000", "8.0" = "$75,000 or more"))

diabetes_data
```


## Splitting Data

```{r, warning=FALSE, message=FALSE}
#library some packages
library(tidymodels)
library(tidyverse)
library(caret)
library(yardstick)
library(glmnet)
library(rpart)
library(parsnip)
library(dials)
library(ranger)
```


```{r}
#split data into training and test set
set.seed(145)
split <- initial_split(diabetes_data, prop = 0.7)
diabetes_train <- training(split)
diabetes_test <- testing(split)

#create 10 cv folds on training data
diabetes_cv <- vfold_cv(diabetes_train, 10)
```


## Logistic Regression Models

Logistic regression models are a type of predictive modeling that can be used for binary classification variables, meaning that there is only two options for the response variable. Logistic regression deals with probabilities of data, so the model will always be between 0 and 1. We will be using logistic regression models to fit the diabetes data because our response variable of Diabetes_binary is binary and we will be able to use the model to predict whether a person has diabetes or not. We will fit three models to compare and determine the best logistic regression model for predicting diabetes. 


### Fit 3 Models

```{r}
#model 1 - Age, Sex, HighBP, HighChol, PhysActivity
lr1_rec <- recipe(Diabetes_binary ~ Age + Sex + HighBP + HighChol + PhysActivity, data = diabetes_train) |>
  step_dummy(Age, Sex, HighBP, HighChol, PhysActivity)
lr1_spec <- logistic_reg() |>
  set_engine("glm")
lr1_wkf <- workflow() |>
  add_recipe(lr1_rec) |>
  add_model(lr1_spec)

#fit lr1 model to cv folds
lr1_fit <- lr1_wkf |>
  fit_resamples(diabetes_cv, metrics = metric_set(mn_log_loss))
```


```{r}
#model 2 - HighBP, HighChol, BMI, Stroke, HeartDiseaseorAttack
lr2_rec <- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + Stroke + HeartDiseaseorAttack, data = diabetes_train) |>
  step_dummy(HighBP, HighChol, Stroke, HeartDiseaseorAttack)
lr2_spec <- logistic_reg() |>
  set_engine("glm")
lr2_wkf <- workflow() |>
  add_recipe(lr2_rec) |>
  add_model(lr2_spec)

#fit lr2 model to cv folds
lr2_fit <- lr2_wkf |>
  fit_resamples(diabetes_cv, metrics = metric_set(mn_log_loss))
```


```{r}
#model 3 - Smoker, Fruits, Veggies, HvyAlcoholConsump
lr3_rec <- recipe(Diabetes_binary ~ Smoker + Fruits + Veggies + HvyAlcoholConsump, data = diabetes_train) |>
  step_dummy(Smoker, Fruits, Veggies, HvyAlcoholConsump)
lr3_spec <- logistic_reg() |>
  set_engine("glm")
lr3_wkf <- workflow() |>
  add_recipe(lr3_rec) |>
  add_model(lr3_spec)

#fit lr3 model to cv folds
lr3_fit <- lr3_wkf |>
  fit_resamples(diabetes_cv, metrics = metric_set(mn_log_loss))
```


### Determine Best Model

```{r}
rbind(lr1_fit |> collect_metrics(),
      lr2_fit |> collect_metrics(),
      lr3_fit |> collect_metrics()) |>
  mutate(Model = c("LR Model 1", "LR Model 2", "LR Model 3"))
```

From the three logistic regression models, the best model is the second model with the lowest log loss metric. 


## Classification Tree

A classification tree is a type of predictive modeling that can be easy to interpret and split into decisions that is made based on predictors. As a decision tree is made, it is split based on the next best predictor to eventually arrive at a final prediction. A classification tree can be used on the diabetes data as there are multiple predictors that a decision tree can use and will be able to come to a prediction based on these predictors. We will create classification trees with different values of cost complexity to determine the best model to fit the data. 


```{r}
#create model workflow
tree_rec <- recipe(Diabetes_binary ~ HighBP + HighChol + Stroke + HeartDiseaseorAttack, data = diabetes_train) |>
  step_dummy(HighBP, HighChol, Stroke, HeartDiseaseorAttack)
tree_spec <- decision_tree(cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_spec)

#tune with various cost complexity
tree_fit <- tree_wkf |>
  tune_grid(resamples = diabetes_cv,
            grid = grid_regular(cost_complexity(), levels = 5),
            metrics = metric_set(mn_log_loss))
```


```{r}
#choose best classification tree model
tree_best_params <- select_best(tree_fit, metric = "mn_log_loss")
tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)
```


## Random Forest

A random forest model is similar to the classification tree as there is a decision tree being made. However, in random forest, many decision trees are made and the predictions are combined to come up with the best prediction of the data. Each tree of the random forest model picks a few features at random to split the decision tree, helping to avoid over fitting the data. We will use a random forest model on the diabetes data to come up with the best predictions. 


```{r}


#create model workflow
rf_rec <- recipe(Diabetes_binary ~ HighBP + HighChol + PhysActivity, data = diabetes_train) |>
  step_dummy(HighBP, HighChol, PhysActivity)
rf_spec <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")
rf_wkf <- workflow() |>
  add_recipe(rf_rec) |>
  add_model(rf_spec)

#tune with mtry
rf_fit <- rf_wkf |>
  tune_grid(resamples = diabetes_cv,
            grid = 5,
            metrics = metric_set(mn_log_loss))
```


```{r}
#choose best random forest model
rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")
rf_final_wkf <- rf_wkf |>
  finalize_workflow(rf_best_params)
```


## Final Model Selection

With the best logistic regression model, the best classification model, and the best random forest model determined, we will compare all the models on the test set to decide which model will be the best overall. 


```{r}

lr_final_fit <- lr2_wkf |>
  last_fit(split, metrics = metric_set(mn_log_loss))
tree_final_fit <- tree_final_wkf |>
  last_fit(split, metrics = metric_set(mn_log_loss))
rf_final_fit <- rf_final_wkf |>
  last_fit(split, metrics = metric_set(mn_log_loss))
```

```{r}
rbind(lr_final_fit |> collect_metrics(),
      tree_final_fit |> collect_metrics(),
      rf_final_fit |> collect_metrics()) |>
  mutate(Model = c("LR Model", "Tree Model", "Random Forest Model"))
```

When testing the best of each type of model using the test data and comparing the log loss, it looks like the best model to predict whether a person has diabetes or not is the logistic regression model. 

```{r}
best_model <- lr2_wkf |>
  fit(diabetes_data)
tidy(best_model)
```

The logistic regression model would be Diabetes_binary ~ -3.02367993 + 0.06461834*BMI + -1.15403139*HighBP + -0.74280903*HighChol + 0.51060086*Stroke + 0.71177963*HeartDiseaseorAttack. 